import os
import pandas as pd
import matplotlib.pyplot as plt
import shap
import xgboost as xgb
from sklearn.ensemble import AdaBoostRegressor
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import r2_score
from tqdm import tqdm
import numpy as np
# ==== è®¾ç½®è·¯å¾„ ====
base_dir = os.getcwd()
#csv_path = os.path.join(base_dir, "data", "data.csv")
csv_path = 'F:/æ–‡ä»¶/æœ¬ç§‘/æ¯•ä¸šè®¾è®¡/æ•°æ®/æ•°æ®åº“-å¡«å…….csv'
#output_dir = os.path.join(base_dir, "results")
output_dir = 'F:/æ–‡ä»¶/æœ¬ç§‘/æ¯•ä¸šè®¾è®¡/SHAP1/'
os.makedirs(output_dir, exist_ok=True)

# ==== 1. åŠ è½½æ•°æ® ====
print("ğŸ“¥ æ­£åœ¨åŠ è½½æ•°æ®...")
df = pd.read_csv(csv_path)
feature_cols = [
        '0.1C',
        '1C',
        #'R','N','C',
        #'å……ç”µç”µå‹','æ”¾ç”µç”µå‹',
        'dope','cover',
        #'ICE',
        'I/I',
        'charge_Li', 'charge_O', 'charge_F',
        'charge_Na', 
        'charge_Mg', 
        'charge_Al', 
        'charge_S', 'charge_K', 'charge_Mn',
        'charge_Fe', 'charge_Co', 'charge_Ni',
        'charge_Zr', 'charge_W']
target_col = 'ICE'
X = df[feature_cols]
y = df[target_col]

# ==== 2. åˆ’åˆ†è®­ç»ƒé›† / æµ‹è¯•é›† ====
print("âœ‚ï¸ åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==== 3. æ¨¡å‹è®­ç»ƒ ====
print("ğŸ‹ï¸â€â™‚ï¸ æ­£åœ¨è®­ç»ƒ XGBoost æ¨¡å‹...")
'''
model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.2,
    subsample=0.8,
    #colsample_bytree=0.8,
    random_state=42
)
'''
model = AdaBoostRegressor(
            n_estimators=200,
            learning_rate=0.5,
            random_state=42
        )
model.fit(X_train, y_train)

# ==== 4. æ¨¡å‹é¢„æµ‹ ====
print("ğŸ”® é¢„æµ‹æµ‹è¯•é›†...")
y_pred = model.predict(X_test)

# ==== 5. SHAP åˆ†æ ====
print("ğŸ§  è®¡ç®— SHAP å€¼...")
#explainer = shap.TreeExplainer(model)
#shap_values = explainer.shap_values(X)

#background = shap.sample(X_train, 50)  # é€šè¿‡é‡‡æ ·å‡å°‘è®¡ç®—é‡
explainer = shap.KernelExplainer(model.predict, X_train, n_jobs=-1)  # ä½¿ç”¨è®­ç»ƒé›†ä½œä¸ºèƒŒæ™¯æ•°æ®
shap_values = explainer.shap_values(X)  # å»ºè®®åœ¨æµ‹è¯•é›†ä¸Šè®¡ç®—ä»¥æå‡é€Ÿåº¦



# ==== 6. å¯è§†åŒ–éƒ¨åˆ† ====
## 6.1 å®é™… vs é¢„æµ‹
plt.figure(figsize=(6, 6))
# è®¡ç®— RÂ² å’Œ RMSE
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
plt.figure(figsize=(6, 6))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.3)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.xlabel("Observed NPP")
plt.ylabel("Predicted NPP")
plt.title("Observed vs Predicted")
# æ˜¾ç¤º RÂ² å’Œ RMSE
plt.text(0.05, 0.95, f"RÂ² = {r2:.4f}", fontsize=12, ha='left', va='top', transform=plt.gca().transAxes)
plt.text(0.05, 0.90, f"RMSE = {rmse:.4f}", fontsize=12, ha='left', va='top', transform=plt.gca().transAxes)
# æ ¼å¼è°ƒæ•´
plt.grid(False)
plt.tight_layout()
# ä¿å­˜å›¾åƒ
plt.savefig(os.path.join(output_dir, "scatter_observed_vs_predicted_with_R2_RMSE.png"))
plt.close()

## 6.2 SHAP èœ‚å·¢å›¾ï¼ˆviolinï¼‰
shap.summary_plot(shap_values, X, plot_type="violin", show=False)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "shap_beeswarm.png"))
plt.close()

## 6.3 SHAP summary å›¾ï¼ˆbarï¼‰
shap.summary_plot(shap_values, X, plot_type="bar", show=False)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "shap_summary_bar.png"))
plt.close()

## 6.4 SHAP Force å›¾ï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
shap.initjs()
force_plot = shap.plots.force(explainer.expected_value, shap_values[0], X.iloc[0], matplotlib=True, show=False)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "shap_force_plot.png"))
plt.close()

## 6.5 SHAP Waterfall å›¾ï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
shap.plots.waterfall(shap.Explanation(values=shap_values[0],
                                      base_values=explainer.expected_value,
                                      data=X.iloc[0].values,
                                      feature_names=feature_cols),
                     show=False)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "shap_waterfall_plot.png"))
plt.close()

## 6.6 SHAP å¤šé¡¹å¼æ‹Ÿåˆä¾èµ–å›¾ï¼ˆæ¯ä¸ªå˜é‡ï¼‰
print("ğŸ“ˆ ç”Ÿæˆå¤šé¡¹å¼æ‹Ÿåˆä¾èµ–å›¾...")
for i, feature in enumerate(tqdm(feature_cols, desc="å¤šé¡¹å¼æ‹Ÿåˆ")):
    shap_val = shap_values[:, i]
    feature_val = X[feature].values

    plt.figure(figsize=(6, 4))
    sns.regplot(
        x=feature_val,
        y=shap_val,
        order=2,
        scatter_kws={'alpha': 0.2, 's': 10},
        line_kws={'color': 'red', 'linewidth': 2}
    )
    plt.xlabel(feature)
    plt.ylabel(f"SHAP value for {feature}")
    plt.title(f"Dependence Plot with Polynomial Fit: {feature}")
    plt.grid(False)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"shap_dependence_poly_{feature}.png"))
    plt.close()

## 6.7 SHAP é»˜è®¤äº¤äº’ä¾èµ–å›¾
print("ğŸ”€ ç”Ÿæˆ SHAP äº¤äº’ä¾èµ–å›¾...")
for feature in tqdm(feature_cols, desc="äº¤äº’ä¾èµ–å›¾"):
    shap.dependence_plot(feature, shap_values, X, show=False)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"shap_dependence_interact_{feature}.png"))
    plt.close()

# ==== 7. è¾“å‡ºæ¨¡å‹æ€§èƒ½ ====
r2 = r2_score(y_test, y_pred)
print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒRÂ² = {r2:.4f}")
print(f"ğŸ“‚ æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°ï¼š{output_dir}")
